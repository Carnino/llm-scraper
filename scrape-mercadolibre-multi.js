import { chromium } from 'playwright'
import { z } from 'zod'
import { openai } from '@ai-sdk/openai'
import LLMScraper from './dist/index.js'
import dotenv from 'dotenv'

dotenv.config()

async function main() {
  try {
    if (!process.env.OPENAI_API_KEY) {
      throw new Error('OPENAI_API_KEY no est√° configurada en el archivo .env')
    }

    console.log('üöÄ Iniciando scraping de m√∫ltiples p√°ginas en Mercado Libre con GPT-4o Mini...')

    const browser = await chromium.launch({ headless: false })
    const page = await browser.newPage()
    const llm = openai.chat('gpt-4o-mini')
    const scraper = new LLMScraper(llm)

    // Definir el esquema para extraer productos de cada p√°gina
    const schema = z.object({
      productos: z.array(
        z.object({
          titulo: z.string().describe('T√≠tulo del producto'),
          precio: z.string().describe('Precio del producto'),
          ubicacion: z.string().optional().describe('Ubicaci√≥n del vendedor'),
          envio_gratis: z.boolean().optional().describe('Si tiene env√≠o gratis'),
          calificacion: z.string().optional().describe('Calificaci√≥n del vendedor'),
          enlace: z.string().describe('Enlace al producto'),
        })
      ).min(1).describe('Productos encontrados en esta p√°gina'),
      pagina_actual: z.number().describe('N√∫mero de p√°gina actual'),
      total_productos: z.number().describe('Total de productos encontrados en esta p√°gina'),
    })

    const todasLasPaginas = []
    const maxPaginas = 20

    for (let pagina = 1; pagina <= maxPaginas; pagina++) {
      try {
        console.log(`\nüìÑ Procesando p√°gina ${pagina} de ${maxPaginas}...`)
        
        // Construir URL con par√°metro de p√°gina
        let url
        if (pagina === 1) {
          url = 'https://www.mercadolibre.com.ar/ofertas?promotion_type=deal_of_the_day#filter_applied=promotion_type&filter_position=3&origin=qcat'
        } else {
          url = `https://www.mercadolibre.com.ar/ofertas?promotion_type=deal_of_the_day&page=${pagina}#filter_applied=promotion_type&filter_position=3&origin=qcat`
        }

        console.log(`üåê Navegando a: ${url}`)
        await page.goto(url, { waitUntil: 'domcontentloaded', timeout: 60000 })

        // Esperar un poco para que la p√°gina cargue completamente
        await page.waitForTimeout(3000)

        console.log(`ü§ñ Ejecutando scraper en p√°gina ${pagina}...`)

        const { data } = await scraper.run(page, schema, {
          format: 'html',
        })

        console.log(`‚úÖ P√°gina ${pagina} completada. Productos encontrados: ${data.total_productos}`)
        
        // Agregar informaci√≥n de la p√°gina a los resultados
        const paginaData = {
          numero_pagina: pagina,
          url: url,
          productos: data.productos,
          total_productos: data.total_productos
        }
        
        todasLasPaginas.push(paginaData)

        // Guardar resultados parciales en un archivo
        const fs = await import('fs')
        fs.writeFileSync(
          `resultados_pagina_${pagina}.json`, 
          JSON.stringify(paginaData, null, 2)
        )

        // Esperar un poco entre p√°ginas para no sobrecargar el servidor
        await page.waitForTimeout(2000)

      } catch (error) {
        console.error(`‚ùå Error en p√°gina ${pagina}:`, error.message)
        // Continuar con la siguiente p√°gina
        continue
      }
    }

    // Guardar todos los resultados en un archivo
    const fs = await import('fs')
    fs.writeFileSync(
      'resultados_completos.json', 
      JSON.stringify({
        total_paginas_procesadas: todasLasPaginas.length,
        fecha_scraping: new Date().toISOString(),
        paginas: todasLasPaginas
      }, null, 2)
    )

    await page.close()
    await browser.close()

    console.log('\nüìä Resumen del scraping:')
    console.log(`‚úÖ P√°ginas procesadas: ${todasLasPaginas.length}`)
    console.log(`üìÅ Archivos generados:`)
    console.log(`   - resultados_completos.json (todos los datos)`)
    for (let i = 1; i <= todasLasPaginas.length; i++) {
      console.log(`   - resultados_pagina_${i}.json`)
    }
    console.log('üéâ ¬°Scraping de m√∫ltiples p√°ginas completado!')

  } catch (error) {
    console.error('‚ùå Error general:', error.message)
    process.exit(1)
  }
}

main() 